<!doctype html>
<html>
<head>
	<title>klab</title>
	<meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="author" content="Jonas Kubilius">
    <meta name="description" content="">
    <meta name="keywords" content="mid-level vision, computer vision, human vision">
    <meta name="google-site-verification" content="k1wISsyM_ADFRsWYOmwrowOMYSYu3MV5afsd-8B2AuQ" />
    <meta content="Building temporal convolutional neural networks: How predictability and parsimony can help each other" property="og:title" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/3.0.3/normalize.min.css" type="text/css"/>
	<link rel="stylesheet" href="https://klab.lt/css/style.css" type="text/css" media="screen" />
	<link rel="shortcut icon" href="https://klab.lt/favicon.ico" />
</head>

<body>
    <div class="page-wrap">
        <header>
            <div class="logo-container">
                <a href="https://klab.lt">
                    <div class="logo"></div>
                    <div class="logo-name">klab</div>
                </a>
                <div class="clear"></div>
                <div class="tagline">&#34;What I cannot create, I do not understand.&#34; â€“ R. Feynman</div>
            </div>

            <nav>
                <ul>                    <li>                        <a href="https://klab.lt/blog/">Blog</a>
                                            </li>                    <li>                        Publications
                        <ul>                            <li><a href="https://klab.lt/publications/papers/">Papers</a></li>                            <li><a href="https://klab.lt/publications/posters/">Posters</a></li>                            <li><a href="https://klab.lt/publications/talks/">Talks</a></li>                            <li><a href="https://klab.lt/publications/figures/">Figures</a></li>                        </ul>
                                            </li>                    <li>                        Education
                        <ul>                            <li><a href="https://klab.lt/education/lectures/">Lectures</a></li>                        </ul>
                                            </li>                    <li>                        About
                        <ul>                            <li><a href="https://klab.lt/about/me/">Me</a></li>                            <li><a href="https://dl.dropboxusercontent.com/u/2498793/Kubilius-CV.pdf">My CV</a></li>                            <li><a href="https://klab.lt/about/media/">Media</a></li>                        </ul>
                                            </li>                </ul>
            </nav>
        </header>

        <div class="main-container">
<div class="content">
    <span class="pub-authors">
        Jonas Kubilius, Kohitij Kar, Daniel L. K. Yamins, James J. DiCarlo
    </span>

    <h1>Building temporal convolutional neural networks: How predictability and parsimony can help each other</h1>

    <span class="pub-journal">
         ICON
    </span>


    <span class="datea">, 2017-08-08 15:00</span>




        <span class="pub-links">
                    <a href="http://www.icon2017.org/program.html">url</a>
              <!--  -->
                    <a href="slides.html">slides</a>
              <!--  -->
    </span>
    <div class="article-content">
         <p>In the recent years, feedforward deep neural networks have surpassed other classes of models in predicting neural responses in the primate inferior temporal (IT) cortex (Yamins et al., 2014) and in providing response patterns consistent with primate behavior in several object judgment tasks (e.g, Kubilius et al., 2016). However, despite a strong promise for bringing better predictive models for many phenomena in cognitive sciences, deep nets remain poorly adopted by cognitive researchers. A common argument against using them is that deep nets are too complex and do not provide an adequate understanding of the processes occurring in the system. In this talk, I will argue that predictability and understanding, or perhaps more concretely, the parsimony of the models that we build are not necessarily inconsistent goals and can in fact enhance each other. To illustrate this idea, I will describe how investigating the performance patterns of object identification in deep nets in our recent work lead to the predictions of response decoding latencies in the monkey IT cortex. Specifically, we observed that the images that deep nets found hard to identify lead to the longer response decoding latencies in the IT cortex, presumably reflecting the lack of recurrent connections in these feedforward architectures. Next, I will demonstrate how we used these empirical observations to inform and constrain new classes of models. I will discuss how we built a general-purpose temporal convolutional neural network architecture that can be defined for any network topology, including within-layer recurrence, feedback and bypass connections. Such a multi-pathway architecture and its time-varying outputs, while built out of the needs of a particular task, also provide a more plausible model of the visual system, opening a possibility for investigating dynamic processes in machines and primates using deep architectures.</p>     </div>

    <div class="clear"></div>
</div>

<div class="clear"></div>
</div>

                <footer>
            (<a href="https://creativecommons.org/licenses/by/4.0/">cc-by</a>) 2012-2018 klab.lt &bull; source code available on <a href="https://github.com/qbilius/klab/" target="_blank">GitHub</a>
        </footer>
            </div>

</body>
</html>