<!doctype html>
<html>
<head>
	<title>klab</title>
	<meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <meta name="author" content="Jonas Kubilius">
    <meta name="description" content="">
    <meta name="keywords" content="mid-level vision, computer vision, human vision">
    <meta name="google-site-verification" content="k1wISsyM_ADFRsWYOmwrowOMYSYu3MV5afsd-8B2AuQ" />
    <meta content="gmin specification (first draft)" property="og:title" />

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/3.0.3/normalize.min.css" type="text/css"/>
	<link rel="stylesheet" href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/css/style.css" type="text/css" media="screen" />
	<link rel="shortcut icon" href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/favicon.ico" />
</head>

<body>
    <div class="page-wrap">
        <header>
            <div class="logo-container">
                <a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content">
                    <div class="logo"></div>
                    <div class="logo-name">klab</div>
                </a>
                <div class="clear"></div>
                <div class="tagline">&#34;What I cannot create, I do not understand.&#34; – R. Feynman</div>
            </div>

            <nav>
                <ul>                    <li>                        <a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/blog/index.html">Blog</a>
                                            </li>                    <li>                        Publications
                        <ul>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/publications/papers/index.html">Papers</a></li>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/publications/posters/index.html">Posters</a></li>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/publications/talks/index.html">Talks</a></li>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/publications/figures/index.html">Figures</a></li>                        </ul>
                                            </li>                    <li>                        Outreach
                        <ul>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/outreach/lectures/index.html">Lectures</a></li>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/outreach/machine-art/index.html">Machine Art</a></li>                        </ul>
                                            </li>                    <li>                        About
                        <ul>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/about/me/index.html">Me</a></li>                            <li><a href="https://dl.dropboxusercontent.com/u/2498793/Kubilius-CV.pdf">My CV</a></li>                            <li><a href="file:////Users/qbilius/Dropbox (Personal)/projects/klab/klab.lt/content/about/media/index.html">Media</a></li>                        </ul>
                                            </li>                </ul>
            </nav>
        </header>

        <div class="main-container">
<div class="content">
    <span class="pub-authors">
        Jonas Kubilius
    </span>

    <h1>gmin specification (first draft)</h1>

    <span class="pub-journal">
             </span>






        <span class="pub-links">
    </span>
    <div class="article-content">
         <h1>Goal</h1>
<p>Image segmentation without relying on stored representations of familiar objects. Here, image segmentation is understood as parsing a scene into (complete) objects and a background with an appropriate depth ordering. Note that this definition is different from a common definition in computer vision literature where segmentation is treated as a <a href="http://en.wikipedia.org/wiki/Image_segmentation">“process of partitioning a digital image into multiple segments”</a>.</p>
<h1>Scope/Assumptions</h1>
<ul>
<li>Only the perceptual part of the visual system is considered (no “vision for action”).</li>
<li>No saccades since we can correctly parse a given scene without moving the eyes no matter where we’re fixating.</li>
<li>Uniform receptive field sizes, which holds approximately true for central vision within a 4 deg radius (Fig. 1a in <a href="http://dx.doi.org/10.1038/nn.2889">Freeman &amp; Simoncelli, 2011</a>). Importantly, this assumption simplifies the problem of dealing with cluttered scenes to one where only a few (possibly incomplete) objects are present at a time.</li>
<li>Grayscale processing (i.e., magnocellular pathway only). Color can be included later by choosing an appropriate transformation (color-opponent channels or the <a href="http://alexholcombe.wordpress.com/2011/07/11/color-space-pictured-and-animated-derrington-krauskopf-lennie/">DKL space</a>; milestone for version 2.0).</li>
<li>No motion for now (milestone for version 3.0).</li>
<li>No 3D reconstructions, only depth ordering of surfaces.</li>
<li>No object recognition (since we’re avoiding top-down effects).</li>
<li>No attention. Coarse segmentation is assumed to occur pre-attentively. Finer segmentation might involve incremental grouping (<a href="http://dx.odi.org/10.1146/annurev.neuro.29.051605.112939">Roelfsema, 2006</a>) but the goal of this model is to provide the initial segmentation.</li>
<li>No explicit task. Segmentation should happen pre-attentively.</li>
</ul>
<h1>Proposed processing sequence</h1>
<p>A similar, yet not as complete, approach has been taken in <a href="http://www.yorku.ca/dregan/index.php?p=books">Regan (2000)</a>, <a href="http://gestaltrevision.be/pdfs/oxford/Self&amp;Roelfsema-The_neural_mechanisms_of_figure-ground_segregation.pdf">Self &amp; Roelfsema (2013)</a>, <a href="http://klab.lt/2013/09/22/perceptual-organization-of-two-dimensional-patterns/" title="Perceptual organization of two-dimensional patterns">Geisler &amp; Super (2000)</a>, <a href="http://dx.doi.org/10.1109/34.868688">Shi &amp; Malik (2000)</a>.</p>
<ol>
<li><strong>Feature detection</strong> Features are detected at every pixel by convolving with appropriate filters. Currently, this step is limited to a convolution with odd and even Gabors at multiple scales for orientation, polarity, and contrast magnitude detection. A maximum is computed over this space to extract features at the best scale.</li>
<li><strong>Center-surround suppression</strong> Since features usually span more than a single pixel, many nearly identical features are detected in a local neighborhood (possibly equivalent to the classical receptive field). This information is redundant and can be coarsened by computing a maximum within a local neighborhood and suppressing the remaining locations (see <a href="http://dx.doi.org/10.1038/nature04977">Sharon et al. (2006)</a> for a similar approach).</li>
<li><strong>Pooling over features (similarity grouping)</strong> Various statistics over the extracted features are computed in the extra-classical receptive field. For example, features could be grouped using the <strong>proximity assumption</strong>: things close in spacetime / feature space are more likely to belong to the same collection (<a href="http://dx.doi.org/10.1162/neco.1991.3.2.194">Földiák, 1991</a>), giving rise to Gestalt grouping principles. <a href="http://www.cns.nyu.edu/lcv/texture/">Portilla &amp; Simoncelli (2000)</a>, <a href="http://dx.doi.org/10.1145/1518701.1518903">Rosenholtz et al. (2009)</a> and <a href="http://dx.doi.org/10.1167/9.12.13">Balas et al. (2009)</a> explored other possible pooling statistics. The outcome of this step is the grouping strength, i.e., a probability of features belonging to the same collection.</li>
<li><strong>More complex feature detection and pooling</strong> The above-described steps could be performed again, at multiple levels of hierarchy, using more complex features (such as curved contours). These steps might be necessary for first- (e.g., orientation-defined) and <a href="http://klab.lt/2013/09/22/perceptual-organization-of-two-dimensional-patterns/" title="Perceptual organization of two-dimensional patterns">second-order grouping displays</a>. For example, in the orientation-defined displays, boundaries at texture discontinuities are detected at this stage of processing.
5.  <strong>Segmentation into objects / collections</strong> So far, grouping has occurred only locally. In the final step, we compute which elements go with which ones globally: if A and B go together, and B and C go together, then A and C go together even if they do not group well directly. This computation could be done by connecting all pairs that group above a certain threshold. Belonging to a collection can mean an increase the firing rate (<a href="http://dx.doi.org/10.1038/nn1304">Roelfsema et al., 2004</a>), an activation of collection units (similar to the <a href="http://en.wikipedia.org/wiki/Grandmother_cell">“grandmother cell”</a> concept), firing synchronization (<a href="http://cogprints.org/1380/">von der Malsburg, 1981</a>) or aligning temporal sequence (<a href="http://dx.doi.org/10.1038/384162a0">Wehr &amp; Laurent, 1996</a>).</li>
<li><strong>Border ownership</strong> Once collections of features are found, border ownership computation can proceed using the <strong>convexity bias assumption</strong>: objects tend to be convex (<a href="http://dx.doi.org/10.1037/a0019076">Kogo et al., 2010</a>).</li>
<li><strong>Depth assignment / occlusion computation</strong> Upon border ownership computation, parts of objects might appear to be missing. These missing parts potentially inform about the depth ordering and are relevant for <em>predicting</em> the possible shape behind the occluder. These predictions might be used later on to refine the segmentation.</li>
</ol>
<p>[caption id=“attachment_747” align=“alignnone” width=“475”]<a href="http://klab.lt/wp-content/uploads/2013/09/architecture_nolabels.png"><img alt="gmin architecture" src="http://klab.lt/wp-content/uploads/2013/09/architecture_nolabels-792x1024.png" title="Proposed architecture (rough sketch)"></a> <strong>Proposed architecture of <em>gmin</em> (rough sketch).</strong> Partial (current) implementation is available on <a href="https://github.com/qbilius/gmin">github</a>.[/caption]</p>     </div>

    <div class="clear"></div>
</div>

<div class="clear"></div>
</div>

                <footer>
            (<a href="https://creativecommons.org/licenses/by/4.0/">cc-by</a>) 2012-2018 klab.lt &bull; source code available on <a href="https://github.com/qbilius/klab/" target="_blank">GitHub</a>
        </footer>
            </div>

</body>
</html>